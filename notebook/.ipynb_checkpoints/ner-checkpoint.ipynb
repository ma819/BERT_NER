{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123235361, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_pickle('pickle/dataset.pickle')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataset['Sentences'].values.tolist()\n",
    "tags = dataset['Tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123235361 123235361\n"
     ]
    }
   ],
   "source": [
    "# cleaned_tags = [re.sub(r'''[',\"\\[\\]]''', \"\", s) for s in tags]\n",
    "\n",
    "# with open('cleaned_tags.pkl', 'wb') as f:\n",
    "#     pickle.dump(cleaned_tags, f)\n",
    "\n",
    "with open('cleaned_tags.pkl', 'rb') as f:\n",
    "    cleaned_tags = pickle.load(f)\n",
    "\n",
    "print(len(sentences), len(cleaned_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = list(set(cleaned_tags))\n",
    "\n",
    "tag2idx = {}\n",
    "for idx, tag in enumerate(unique_tags):\n",
    "    tag2idx[tag] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1805258it [00:07, 288972.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2487122it [00:10, 131836.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3240080it [00:11, 663371.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3527263it [00:11, 668662.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4686564it [00:14, 715135.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5286490it [00:17, 404305.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5683454it [00:17, 684726.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6377357it [00:18, 732522.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6529093it [00:18, 724528.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6819593it [00:20, 279013.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8530300it [00:23, 742572.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9525771it [00:26, 672135.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9754910it [00:26, 726178.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9991767it [00:26, 764251.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10302878it [00:27, 770048.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10615012it [00:27, 775052.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11873162it [00:30, 501775.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12265185it [00:31, 715656.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12659628it [00:31, 772733.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16514327it [00:38, 749063.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17341979it [00:39, 701352.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17736682it [00:40, 767743.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18924287it [00:41, 788592.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19080991it [00:43, 169750.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19388533it [00:44, 410662.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20254981it [00:45, 768753.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20868940it [00:46, 727036.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21862675it [00:47, 752850.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22561520it [00:48, 768816.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23182182it [00:49, 747514.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24152573it [00:51, 383525.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24352536it [00:53, 173330.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24762750it [00:54, 497840.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24971519it [00:54, 605857.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27068803it [00:57, 769548.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27374035it [00:57, 746392.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27678910it [00:58, 751679.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27993152it [00:58, 763381.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28449499it [00:59, 733421.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29075773it [01:00, 776992.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31905287it [01:06, 743795.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33224394it [01:07, 758481.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33538349it [01:08, 770231.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33928375it [01:08, 774428.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34390236it [01:09, 758302.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35091142it [01:10, 769477.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36650408it [01:12, 771904.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37038162it [01:12, 759293.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37965382it [01:14, 736500.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38419778it [01:14, 719454.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39154312it [01:22, 286605.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40803526it [01:24, 703268.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42242080it [01:26, 776974.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42472201it [01:26, 746135.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43096063it [01:27, 773300.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43879129it [01:28, 779445.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44193249it [01:29, 760164.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44961865it [01:30, 753099.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45357790it [01:30, 780944.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47041943it [01:32, 759780.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47357034it [01:33, 777585.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48774649it [01:38, 126498.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50294936it [01:40, 752604.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51287892it [01:41, 761001.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51521283it [01:41, 772093.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52485405it [01:43, 683372.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54472799it [01:45, 752657.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54774056it [01:46, 719575.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55365671it [01:47, 741758.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55592977it [01:47, 749927.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59901070it [01:54, 644473.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60453095it [01:55, 692126.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60965750it [01:55, 711969.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62156417it [02:23, 557262.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63153766it [02:24, 763578.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63693255it [02:25, 768469.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63925347it [02:25, 766942.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65155605it [02:27, 752790.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68434304it [02:31, 748952.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69577751it [02:33, 729686.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72380788it [02:37, 749882.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72981966it [02:37, 740478.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73279250it [02:38, 734569.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75307122it [02:41, 740636.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75978857it [02:42, 732098.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76425157it [02:42, 735862.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76713153it [03:23, 15144.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78082315it [03:25, 719964.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78237241it [03:25, 744269.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79829624it [03:27, 749419.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80055101it [03:28, 742776.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80875480it [03:29, 736473.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82747001it [03:31, 752822.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85696718it [03:35, 721926.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85838570it [03:36, 684640.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86365672it [03:37, 601835.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87416679it [03:38, 722602.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87633261it [03:38, 718088.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87847877it [03:39, 709513.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89415477it [03:41, 652950.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90616969it [03:43, 709574.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91553480it [03:44, 662556.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91909822it [03:45, 688020.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92179536it [03:45, 660742.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92672983it [03:46, 691279.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93035132it [03:46, 716099.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94389390it [03:48, 715510.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95165163it [03:49, 692405.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95745287it [03:50, 716139.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96059157it [04:43, 8236.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97229566it [04:45, 558365.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97592679it [04:45, 693953.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97951717it [04:46, 692312.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98245953it [04:46, 725043.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99091472it [04:47, 663993.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99290320it [04:48, 644172.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99981523it [04:49, 693209.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100791437it [04:50, 721445.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102545439it [04:52, 662072.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102827791it [04:53, 690117.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103186965it [04:53, 713474.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103332178it [04:53, 716796.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104226641it [04:55, 647785.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105673516it [04:57, 676378.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106026971it [04:57, 706382.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106323363it [04:58, 728228.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "107261070it [04:59, 716727.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "107689336it [05:00, 710816.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108422747it [05:01, 730258.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109371868it [05:02, 704797.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110583961it [05:04, 679198.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110949770it [05:04, 724097.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111813117it [05:06, 692625.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112295761it [05:06, 679473.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113352129it [05:08, 693729.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113781009it [05:08, 710527.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115138357it [05:10, 709289.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116055750it [05:12, 705633.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117783522it [05:14, 590208.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120153630it [06:32, 16740.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120576669it [06:32, 129332.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120716966it [06:33, 218317.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121487248it [06:34, 677719.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121776027it [06:34, 711584.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "122267057it [06:35, 659009.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "122666426it [06:35, 664626.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123235361it [06:36, 310643.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n",
      "Error in re for token: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_list = []\n",
    "token_tag_list = []\n",
    "sentence = []\n",
    "token_tag = []\n",
    "for token, tag in tqdm(zip(sentences, cleaned_tags)):\n",
    "    sentence.append(token)\n",
    "    token_tag.append(tag)\n",
    "    try:\n",
    "        if bool(re.match(r\"[.]\", token)):\n",
    "            if len(sentence) >= 4:\n",
    "                sentences_list.append(sentence)\n",
    "                token_tag_list.append(token_tag)\n",
    "                sentence = []\n",
    "                token_tag = []\n",
    "    except:\n",
    "        print(f\"Error in re for token: {token}\")\n",
    "        sentences_list.append(sentence)\n",
    "        token_tag_list.append(token_tag)\n",
    "        sentence = []\n",
    "        token_tag = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5911818/5911818 [00:02<00:00, 2583444.99it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences_length = []\n",
    "for val in tqdm(sentences_list):\n",
    "    sentences_length.append(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:01, 821.09it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences_list = []\n",
    "tokenized_sent_tag_list = []\n",
    "for token_list, tag_list in tqdm(zip(sentences_list, token_tag_list)):\n",
    "    updated_token_list = []\n",
    "    updated_tag_list = []\n",
    "    for token, tag in zip(token_list, tag_list):\n",
    "        try:\n",
    "            tokenized_list = tokenizer.tokenize(token)\n",
    "            updated_token_list.extend(tokenized_list)\n",
    "            updated_tag_list.extend([tag2idx[tag]]*len(tokenized_list))\n",
    "        except:\n",
    "            print(f\"Tokenizatin failed for token: {token}\")\n",
    "    tokenized_sentences_list.append(updated_token_list)\n",
    "    tokenized_sent_tag_list.append(updated_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Tokenized Data\n",
    "with open('pickle/tokenized_sentences_list.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_sentences_list, f)\n",
    "with open('pickle/tokenized_sent_tag_list.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_sent_tag_list, f)\n",
    "    \n",
    "# # Loading Processed Tokenized Data\n",
    "# with open('pickle/tokenized_sentences_list.pkl', 'rb') as f:\n",
    "#     tokenized_sentences_list = pickle.load(f)\n",
    "    \n",
    "# with open('pickle/tokenized_sent_tag_list.pkl', 'rb') as f:\n",
    "#     tokenized_sent_tag_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 55714.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Mapping tokens to ids\n",
    "input_ids = []\n",
    "for tokenized_sentence in tqdm(tokenized_sentences_list):\n",
    "    input_ids.append(tokenizer.convert_tokens_to_ids(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids), len(tokenized_sent_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 21211.74it/s]\n"
     ]
    }
   ],
   "source": [
    "attention_mask = []\n",
    "for input_ in tqdm(input_ids):\n",
    "    attention_mask.append(torch.ones(len(input_)))\n",
    "\n",
    "padded_attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "    attention_mask, \n",
    "    batch_first=True, \n",
    "    padding_value=0.0)\n",
    "\n",
    "padded_input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "    [torch.tensor(input_) for input_ in input_ids], \n",
    "    batch_first=True, \n",
    "    padding_value=0.0)\n",
    "\n",
    "padded_tags = torch.nn.utils.rnn.pad_sequence(\n",
    "    [torch.tensor(tag_) for tag_ in tokenized_sent_tag_list], \n",
    "    batch_first=True, \n",
    "    padding_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERBert(nn.Module):\n",
    "    \n",
    "    def __init__(self, tag_count=4):\n",
    "        super(NERBert, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, tag_count)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        output = self.bert(input_ids, attention_mask=attention_mask) # Model gives 'last_hidden_state' and 'pooler_output'\n",
    "        \n",
    "        pre_classifier_layer = self.dropout(output.last_hidden_state)\n",
    "        model_output = self.classifier(pre_classifier_layer)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, temp_tokens, train_tags, temp_tags, train_mask, temp_mask =  train_test_split(\n",
    "    padded_input_ids, padded_tags, padded_attention_mask,\n",
    "    random_state=2018, \n",
    "    test_size=0.3\n",
    ")\n",
    "\n",
    "val_tokens, test_tokens, val_tags, test_tags, val_mask, test_mask = train_test_split(\n",
    "    temp_tokens, temp_tags, temp_mask,\n",
    "    random_state=2018, \n",
    "    test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_tokens, train_mask, train_tags)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_tokens, val_mask, val_tags)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_criteria, train_dataloader):\n",
    "    try:\n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_logits = []\n",
    "        \n",
    "        # iterate over batches\n",
    "        for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "            # progress update after every 50 batches.\n",
    "            if step % 50 == 0 and not step == 0:\n",
    "                print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "            # push the batch to gpu\n",
    "            batch = [r.to(device) for r in batch]\n",
    "\n",
    "            sent_id, mask, labels = batch\n",
    "\n",
    "            # clear previously calculated gradients \n",
    "            model.zero_grad()        \n",
    "\n",
    "            # get model predictions for the current batch\n",
    "            logits = model(sent_id, mask)\n",
    "\n",
    "            # compute the loss between actual and predicted values\n",
    "            loss = loss_criteria(logits.permute(0, 2, 1), labels)\n",
    "\n",
    "            # add on to the total loss\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            # backward pass to calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # model predictions are stored on GPU. So, push it to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            # append the model predictions\n",
    "            total_logits.append(logits)\n",
    "            \n",
    "        # compute the training loss of the epoch\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        total_logits = np.concatenate(total_logits, axis=0)\n",
    "        \n",
    "\n",
    "        return avg_loss, total_logits\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training the model on line: {sys.exc_info()[2].tb_lineno}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(model, val_dataloader, loss_criteria):\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_logits = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "            # Calculate elapsed time in minutes.\n",
    "#             elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            logits = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = loss_criteria(logits.permute(0, 2, 1),labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            total_logits.append(logits)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_logits  = np.concatenate(total_logits, axis=0)\n",
    "\n",
    "    return avg_loss, total_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NERBert(tag_count=len(tag2idx))\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5) # learning rate\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.271\n",
      "Validation Loss: 0.642\n",
      "\n",
      " Epoch 2 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.484\n",
      "Validation Loss: 0.319\n",
      "\n",
      " Epoch 3 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.299\n",
      "Validation Loss: 0.276\n",
      "\n",
      " Epoch 4 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.262\n",
      "Validation Loss: 0.250\n",
      "\n",
      " Epoch 5 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.234\n",
      "Validation Loss: 0.224\n",
      "\n",
      " Epoch 6 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.208\n",
      "Validation Loss: 0.202\n",
      "\n",
      " Epoch 7 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.183\n",
      "Validation Loss: 0.185\n",
      "\n",
      " Epoch 8 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.164\n",
      "Validation Loss: 0.177\n",
      "\n",
      " Epoch 9 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.147\n",
      "Validation Loss: 0.166\n",
      "\n",
      " Epoch 10 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.134\n",
      "Validation Loss: 0.161\n"
     ]
    }
   ],
   "source": [
    "epochs=50\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train(\n",
    "        model = model, \n",
    "        optimizer = optimizer, \n",
    "        loss_criteria = criterion, \n",
    "        train_dataloader = train_dataloader \n",
    "        \n",
    "    )\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate(\n",
    "        model = model, \n",
    "        val_dataloader = val_dataloader, \n",
    "        loss_criteria = criterion\n",
    "    )\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'trained_weights/saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_from_logits(logits):\n",
    "    try:\n",
    "        tag_prob = nn.Softmax(dim=2)(logits)\n",
    "        tag_prediction = torch.argmax(tag_prob, dim=2).detach().cpu().numpy()\n",
    "        return tag_prediction\n",
    "    except Exception as e:\n",
    "        print(f\"Error in line: {sys.exc_info()[2].tb_lineno}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_result(tag2idx, c_tag_id):\n",
    "    try:\n",
    "        prediction_result = []\n",
    "        for sent_ in c_tag_id:\n",
    "            prediction_result.append(\n",
    "                list(map(lambda x: list(tag2idx.keys())[list(tag2idx.values()).index(x)], sent_))\n",
    "            )\n",
    "            \n",
    "        tagged_entity = np.concatenate(prediction_result, axis=0)\n",
    "        return tagged_entity\n",
    "    except Exception as e:\n",
    "        print(f\"Error in line: {sys.exc_info()[2].tb_lineno}\")\n",
    "        print(e)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'trained_weights/saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    logits = model(test_tokens.to(device), test_mask.to(device))\n",
    "    preds = get_prediction_from_logits(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags = test_tags.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags = classification_result(\n",
    "    tag2idx = tag2idx, \n",
    "    c_tag_id = test_tags\n",
    ")\n",
    "preds = classification_result(\n",
    "    tag2idx = tag2idx, \n",
    "    c_tag_id = preds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9900,), (9900,))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tags.shape, preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "    B-LOCATION       0.30      0.37      0.33        57\n",
      "        B-MISC       0.40      0.32      0.36       212\n",
      "B-ORGANIZATION       0.00      0.00      0.00        16\n",
      "      B-PERSON       0.51      0.59      0.55       133\n",
      "    I-LOCATION       0.00      0.00      0.00        24\n",
      "        I-MISC       1.00      0.99      0.99      5593\n",
      "I-ORGANIZATION       0.00      0.00      0.00         1\n",
      "      I-PERSON       0.47      0.34      0.40        53\n",
      "             O       0.94      0.96      0.95      3811\n",
      "\n",
      "      accuracy                           0.95      9900\n",
      "     macro avg       0.40      0.40      0.40      9900\n",
      "  weighted avg       0.95      0.95      0.95      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.squeeze(test_tags.reshape(1, -1)), np.squeeze(preds.reshape(1, -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
